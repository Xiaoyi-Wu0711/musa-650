{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## HW4 - EuroSAT Land Use and Land Cover Classification using Deep Learning\n",
    " \n",
    "In this homework your task is to implement deep learning models to solve a typical problem in satellite imaging  using a benchmark dataset. The homework was designed to make you work on increasingly more complex models. We hope that the homework will be very helpful to improve your skills and knowledge in deep learning!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S1:\n",
    "\n",
    "- Visit the EuroSAT data description page and download the data: https://github.com/phelber/eurosat\n",
    "\n",
    "- Split the data into training (50%) and testing sets (50%), stratified on class labels (equal percentage of each class type in train and test sets).\n",
    "\n",
    "- Convert each RGB image to grayscale and flatten the images into a data matrix (n x p: n = #samples, p = #pixels in each image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cluster\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Flatten,Conv2D, MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64)\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY = r'EuroSAT\\2750'\n",
    "\n",
    "CATEGORIES = ['SeaLake', 'River','Residential','PermanentCrop',\n",
    "              'Pasture','Industrial','Highway','HerbaceousVegetation',\n",
    "             'Forest','AnnualCrop']\n",
    "num_class=(len(CATEGORIES))\n",
    "image=PIL.Image.open('EuroSAT/2750/Forest/Forest_1.jpg',mode='r', formats=None)\n",
    "pix=image.load()\n",
    "print(image.size)\n",
    "w=image.size[0]\n",
    "h=image.size[1]\n",
    "n_spectrum = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "X=[]\n",
    "Y=[]\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        label = CATEGORIES.index(category)\n",
    "        arr = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        new_arr = cv2.resize(arr, (64, 64))\n",
    "        X.append(new_arr)\n",
    "        Y.append(label)\n",
    "        data.append([new_arr, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_value=np.array(X)\n",
    "X_value=X_value.reshape(X_value.shape[0],-1)\n",
    "X_value=X_value.astype('float32')\n",
    "X_value/=255\n",
    "Y_value=np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_value, Y_value, stratify = Y_value, train_size = 0.5, random_state=42)\n",
    "y_train = to_categorical(y_train, num_class)\n",
    "y_test = to_categorical(y_test, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000,)\n",
      "(0,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13500, 4096)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(Y_value[(Y_value==2)].shape)\n",
    "print(y_train[(y_train==2)].shape)\n",
    "batch_size=(x_train.shape)\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S2:\n",
    "\n",
    "- Implement a first deep learning model (M.1) using a fully connected network with a single fully connected layer (i.e: input layer + fully connected layer as the output layer). \n",
    "\n",
    "###  CNN:\n",
    "1. reduce para: kernel sharing para, proximity in pixel, kernel is learned by the DNN\n",
    "\n",
    "Q2.1: Calculate classification accuracy on the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,970\n",
      "Trainable params: 40,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(num_class, activation='softmax', input_shape=(batch_size[1],)))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_1.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                      # unbiased sampling, but large the size the lower variance\n",
    "                      # 32->64\n",
    "                    epochs=50,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.2163143157958984\n",
      "Test accuracy: 0.21607407927513123\n"
     ]
    }
   ],
   "source": [
    "score = model_1.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3:\n",
    "\n",
    "- Implement a second deep learning model (M.2) adding an additional fully connected hidden layer (with an arbitrary number of nodes) to the previous model. \n",
    "\n",
    "\n",
    "Q3.1: Calculate classification accuracy on the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               2097664   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,102,794\n",
      "Trainable params: 2,102,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(512, activation='relu', input_shape=(batch_size[1],)))\n",
    "model_2.add(Dense(num_class, activation='softmax'))\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "211/211 [==============================] - 6s 27ms/step - loss: 2.8946 - accuracy: 0.1164 - val_loss: 2.3006 - val_accuracy: 0.0959\n",
      "Epoch 2/10\n",
      "211/211 [==============================] - 6s 26ms/step - loss: 2.2485 - accuracy: 0.1409 - val_loss: 2.1950 - val_accuracy: 0.1385\n",
      "Epoch 3/10\n",
      "211/211 [==============================] - 5s 26ms/step - loss: 2.1863 - accuracy: 0.1734 - val_loss: 2.1338 - val_accuracy: 0.2035\n",
      "Epoch 4/10\n",
      "211/211 [==============================] - 6s 26ms/step - loss: 2.1405 - accuracy: 0.2039 - val_loss: 2.1403 - val_accuracy: 0.1927\n",
      "Epoch 5/10\n",
      "211/211 [==============================] - 6s 26ms/step - loss: 2.0994 - accuracy: 0.2353 - val_loss: 2.1026 - val_accuracy: 0.2350\n",
      "Epoch 6/10\n",
      "211/211 [==============================] - 6s 26ms/step - loss: 2.0660 - accuracy: 0.2526 - val_loss: 2.0115 - val_accuracy: 0.3034\n",
      "Epoch 7/10\n",
      "211/211 [==============================] - 6s 26ms/step - loss: 2.0277 - accuracy: 0.2612 - val_loss: 1.9958 - val_accuracy: 0.2938\n",
      "Epoch 8/10\n",
      "211/211 [==============================] - 5s 26ms/step - loss: 1.9980 - accuracy: 0.2793 - val_loss: 2.0360 - val_accuracy: 0.2240\n",
      "Epoch 9/10\n",
      "211/211 [==============================] - 5s 26ms/step - loss: 1.9744 - accuracy: 0.2879 - val_loss: 1.9818 - val_accuracy: 0.2470\n",
      "Epoch 10/10\n",
      "211/211 [==============================] - 6s 26ms/step - loss: 1.9469 - accuracy: 0.2972 - val_loss: 1.9432 - val_accuracy: 0.2984\n"
     ]
    }
   ],
   "source": [
    "history = model_2.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.9432449340820312\n",
      "Test accuracy: 0.298370361328125\n"
     ]
    }
   ],
   "source": [
    "score = model_2.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S4:\n",
    "\n",
    "- Implement a third deep learning model (M.3) adding two additional fully connected hidden layers (with arbitrary number of nodes) for a total of four, as well as drop-out layers to the previous model. \n",
    "\n",
    "#todo: how many layers in total\n",
    "\n",
    "Q4.1: Calculate classification accuracy on the test data.\n",
    "\n",
    "Q4.2: Compare against previous models. Which model was the \"best\"? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 216)               110808    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                13888     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,227,170\n",
      "Trainable params: 2,227,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# softmax: multi-class prob\n",
    "# relu: positive value \n",
    "model_3 = Sequential()\n",
    "model_3.add(Dense(512, activation='relu', input_shape=(batch_size[1],)))\n",
    "model_3.add(Dense(216, activation='relu'))\n",
    "model_3.add(Dense(64, activation='relu'))\n",
    "model_3.add(Dropout(0.25))\n",
    "model_3.add(Dense(num_class, activation='softmax'))\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "211/211 [==============================] - 7s 29ms/step - loss: 2.3488 - accuracy: 0.1087 - val_loss: 2.2527 - val_accuracy: 0.1130\n",
      "Epoch 2/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 2.2322 - accuracy: 0.1532 - val_loss: 2.1751 - val_accuracy: 0.2103\n",
      "Epoch 3/50\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 2.1444 - accuracy: 0.2046 - val_loss: 2.1112 - val_accuracy: 0.2281\n",
      "Epoch 4/50\n",
      "211/211 [==============================] - 6s 31ms/step - loss: 2.0348 - accuracy: 0.2415 - val_loss: 1.9602 - val_accuracy: 0.2501\n",
      "Epoch 5/50\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 1.9403 - accuracy: 0.2691 - val_loss: 1.8702 - val_accuracy: 0.2826\n",
      "Epoch 6/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.8797 - accuracy: 0.2918 - val_loss: 1.7940 - val_accuracy: 0.3278\n",
      "Epoch 7/50\n",
      "211/211 [==============================] - 6s 27ms/step - loss: 1.8368 - accuracy: 0.3084 - val_loss: 1.7474 - val_accuracy: 0.3409\n",
      "Epoch 8/50\n",
      "211/211 [==============================] - 6s 26ms/step - loss: 1.8171 - accuracy: 0.3126 - val_loss: 1.8468 - val_accuracy: 0.2890\n",
      "Epoch 9/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.7926 - accuracy: 0.3196 - val_loss: 2.0998 - val_accuracy: 0.2139\n",
      "Epoch 10/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.7947 - accuracy: 0.3219 - val_loss: 1.7549 - val_accuracy: 0.3211\n",
      "Epoch 11/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.7779 - accuracy: 0.3241 - val_loss: 1.7621 - val_accuracy: 0.3492\n",
      "Epoch 12/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.7630 - accuracy: 0.3357 - val_loss: 1.7439 - val_accuracy: 0.3304\n",
      "Epoch 13/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.7483 - accuracy: 0.3403 - val_loss: 1.7580 - val_accuracy: 0.3166\n",
      "Epoch 14/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.7421 - accuracy: 0.3406 - val_loss: 1.7592 - val_accuracy: 0.3233\n",
      "Epoch 15/50\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 1.7379 - accuracy: 0.3430 - val_loss: 1.7310 - val_accuracy: 0.3296\n",
      "Epoch 16/50\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 1.7311 - accuracy: 0.3464 - val_loss: 1.8084 - val_accuracy: 0.2967\n",
      "Epoch 17/50\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 1.7220 - accuracy: 0.3496 - val_loss: 1.6839 - val_accuracy: 0.3603\n",
      "Epoch 18/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.7103 - accuracy: 0.3550 - val_loss: 1.8294 - val_accuracy: 0.2979\n",
      "Epoch 19/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.7082 - accuracy: 0.3531 - val_loss: 1.6968 - val_accuracy: 0.3595\n",
      "Epoch 20/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.6963 - accuracy: 0.3606 - val_loss: 1.7010 - val_accuracy: 0.3507\n",
      "Epoch 21/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.6912 - accuracy: 0.3620 - val_loss: 1.7531 - val_accuracy: 0.3233\n",
      "Epoch 22/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.6844 - accuracy: 0.3627 - val_loss: 1.6604 - val_accuracy: 0.3660\n",
      "Epoch 23/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.6743 - accuracy: 0.3681 - val_loss: 1.9096 - val_accuracy: 0.2794\n",
      "Epoch 24/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.6709 - accuracy: 0.3640 - val_loss: 1.6852 - val_accuracy: 0.3549\n",
      "Epoch 25/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.6582 - accuracy: 0.3699 - val_loss: 1.6867 - val_accuracy: 0.3483\n",
      "Epoch 26/50\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 1.6610 - accuracy: 0.3770 - val_loss: 1.6540 - val_accuracy: 0.3579\n",
      "Epoch 27/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.6516 - accuracy: 0.3785 - val_loss: 1.6537 - val_accuracy: 0.3659\n",
      "Epoch 28/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.6490 - accuracy: 0.3775 - val_loss: 1.7628 - val_accuracy: 0.3559\n",
      "Epoch 29/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.6425 - accuracy: 0.3815 - val_loss: 1.6325 - val_accuracy: 0.3763\n",
      "Epoch 30/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.6386 - accuracy: 0.3787 - val_loss: 1.7053 - val_accuracy: 0.3541\n",
      "Epoch 31/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.6245 - accuracy: 0.3911 - val_loss: 1.6494 - val_accuracy: 0.3637\n",
      "Epoch 32/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.6308 - accuracy: 0.3797 - val_loss: 1.6228 - val_accuracy: 0.3731\n",
      "Epoch 33/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.6135 - accuracy: 0.3910 - val_loss: 1.6882 - val_accuracy: 0.3561\n",
      "Epoch 34/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.6167 - accuracy: 0.3830 - val_loss: 1.6613 - val_accuracy: 0.3561\n",
      "Epoch 35/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.6047 - accuracy: 0.3926 - val_loss: 1.6403 - val_accuracy: 0.3699\n",
      "Epoch 36/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.6077 - accuracy: 0.3920 - val_loss: 1.7409 - val_accuracy: 0.3339\n",
      "Epoch 37/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.5992 - accuracy: 0.3933 - val_loss: 1.6918 - val_accuracy: 0.3676\n",
      "Epoch 38/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.6039 - accuracy: 0.3923 - val_loss: 1.7166 - val_accuracy: 0.3385\n",
      "Epoch 39/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.5861 - accuracy: 0.3970 - val_loss: 1.6653 - val_accuracy: 0.3527\n",
      "Epoch 40/50\n",
      "211/211 [==============================] - 6s 28ms/step - loss: 1.5900 - accuracy: 0.3933 - val_loss: 1.7621 - val_accuracy: 0.3404\n",
      "Epoch 41/50\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 1.5863 - accuracy: 0.3967 - val_loss: 1.5971 - val_accuracy: 0.3913\n",
      "Epoch 42/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.5849 - accuracy: 0.3990 - val_loss: 1.5907 - val_accuracy: 0.3908\n",
      "Epoch 43/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.5790 - accuracy: 0.4015 - val_loss: 1.6708 - val_accuracy: 0.3567\n",
      "Epoch 44/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.5755 - accuracy: 0.4025 - val_loss: 1.7267 - val_accuracy: 0.3347\n",
      "Epoch 45/50\n",
      "211/211 [==============================] - 6s 30ms/step - loss: 1.5687 - accuracy: 0.4033 - val_loss: 1.7189 - val_accuracy: 0.3473\n",
      "Epoch 46/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.5713 - accuracy: 0.4019 - val_loss: 1.6259 - val_accuracy: 0.3931\n",
      "Epoch 47/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.5657 - accuracy: 0.4044 - val_loss: 1.6131 - val_accuracy: 0.3890\n",
      "Epoch 48/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.5669 - accuracy: 0.4057 - val_loss: 1.6651 - val_accuracy: 0.3603\n",
      "Epoch 49/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.5617 - accuracy: 0.4059 - val_loss: 1.5685 - val_accuracy: 0.4012\n",
      "Epoch 50/50\n",
      "211/211 [==============================] - 6s 29ms/step - loss: 1.5612 - accuracy: 0.4038 - val_loss: 1.7739 - val_accuracy: 0.3384\n"
     ]
    }
   ],
   "source": [
    "history = model_3.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.773932695388794\n",
      "Test accuracy: 0.3383703827857971\n"
     ]
    }
   ],
   "source": [
    "score = model_3.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S5:\n",
    "\n",
    "- Take the original RGB images and do not vectorize them. Use these images as the data input for the following models (M.4 and M.5).\n",
    "- Implement a fourth CNN model (M.4) that includes the following layers: Conv2D, MaxPooling2D, Dropout, Flatten, Dense. \n",
    "\n",
    "Q5.1: Calculate classification accuracy on the test data.\n",
    "\n",
    "Q5.2: Compare against previous models. Which model was the \"best\"? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "X=[]\n",
    "Y=[]\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        label = CATEGORIES.index(category)\n",
    "        arr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        new_arr = cv2.resize(arr, (64, 64))\n",
    "        X.append(new_arr)\n",
    "        Y.append(label)\n",
    "        data.append([new_arr, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27000, 64, 64, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_value=X.astype('float32')\n",
    "X_value/=255\n",
    "Y_value=np.array(Y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_value, Y_value, stratify = Y_value, train_size = 0.5, random_state=42)\n",
    "y_train = to_categorical(y_train, num_class)\n",
    "y_test = to_categorical(y_test, num_class)\n",
    "\n",
    "input_shape=(64,64,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 31, 31, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 31, 31, 32)        0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 30752)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                307530    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 308,426\n",
      "Trainable params: 308,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4 = Sequential()\n",
    "\n",
    "model_4.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',input_shape=input_shape\n",
    "                ))\n",
    "\n",
    "model_4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_4.add(Dropout(0.25))\n",
    "model_4.add(Flatten())\n",
    "model_4.add(Dense(num_class, activation='softmax'))\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "211/211 [==============================] - 16s 76ms/step - loss: 1.6132 - accuracy: 0.4103 - val_loss: 1.3055 - val_accuracy: 0.4965\n",
      "Epoch 2/10\n",
      "211/211 [==============================] - 16s 74ms/step - loss: 1.2435 - accuracy: 0.5479 - val_loss: 1.1634 - val_accuracy: 0.5776\n",
      "Epoch 3/10\n",
      "211/211 [==============================] - 16s 75ms/step - loss: 1.0984 - accuracy: 0.6079 - val_loss: 1.3685 - val_accuracy: 0.4753\n",
      "Epoch 4/10\n",
      "211/211 [==============================] - 16s 76ms/step - loss: 1.0173 - accuracy: 0.6431 - val_loss: 1.0135 - val_accuracy: 0.6250\n",
      "Epoch 5/10\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.9674 - accuracy: 0.6659 - val_loss: 0.9851 - val_accuracy: 0.6285\n",
      "Epoch 6/10\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.8872 - accuracy: 0.6925 - val_loss: 0.9586 - val_accuracy: 0.6673\n",
      "Epoch 7/10\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.8563 - accuracy: 0.7022 - val_loss: 0.8896 - val_accuracy: 0.7010\n",
      "Epoch 8/10\n",
      "211/211 [==============================] - 16s 77ms/step - loss: 0.8090 - accuracy: 0.7261 - val_loss: 0.9031 - val_accuracy: 0.6900\n",
      "Epoch 9/10\n",
      "211/211 [==============================] - 16s 76ms/step - loss: 0.7703 - accuracy: 0.7373 - val_loss: 1.0022 - val_accuracy: 0.6423\n",
      "Epoch 10/10\n",
      "211/211 [==============================] - 16s 76ms/step - loss: 0.7513 - accuracy: 0.7479 - val_loss: 1.0190 - val_accuracy: 0.6319\n",
      "Test loss: 1.0190004110336304\n",
      "Test accuracy: 0.6319259405136108\n"
     ]
    }
   ],
   "source": [
    "history = model_4.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model_4.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S6:\n",
    "\n",
    "- Using RGB images from S5, implement a fifth deep learning model (M.5) targeting accuracy that will outperform all previous models. You are free to use any tools and techniques, as well as pre-trained models for transfer learning. \n",
    "\n",
    "Q6.1: Describe the model you built, and why you chose it.\n",
    "\n",
    "Q6.2: Calculate classification accuracy on the test data.\n",
    "\n",
    "Q6.3: Compare against previous models. Which model was the \"best\"? Why?\n",
    "\n",
    "Q6.4: What are the two classes with the highest labeling error? Explain using data and showing mis-classified examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 31, 31, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 31, 31, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                92170     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 148,490\n",
      "Trainable params: 148,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_5 = Sequential()\n",
    "model_5.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "model_5.add(MaxPooling2D((2, 2)))\n",
    "model_5.add(Dropout(0.25))\n",
    "model_5.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_5.add(MaxPooling2D((2, 2)))\n",
    "model_5.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_5.add(Flatten())\n",
    "model_5.add(Dense(num_class, activation='softmax'))\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "211/211 [==============================] - 31s 146ms/step - loss: 1.6961 - accuracy: 0.3583 - val_loss: 1.4470 - val_accuracy: 0.4362\n",
      "Epoch 2/10\n",
      "211/211 [==============================] - 32s 150ms/step - loss: 1.2250 - accuracy: 0.5611 - val_loss: 1.0498 - val_accuracy: 0.5990\n",
      "Epoch 3/10\n",
      "211/211 [==============================] - 32s 152ms/step - loss: 1.0013 - accuracy: 0.6429 - val_loss: 1.2746 - val_accuracy: 0.5451\n",
      "Epoch 4/10\n",
      "211/211 [==============================] - 33s 155ms/step - loss: 0.8809 - accuracy: 0.6790 - val_loss: 0.8854 - val_accuracy: 0.6628\n",
      "Epoch 5/10\n",
      "211/211 [==============================] - 33s 155ms/step - loss: 0.7959 - accuracy: 0.7105 - val_loss: 0.7491 - val_accuracy: 0.7120\n",
      "Epoch 6/10\n",
      "211/211 [==============================] - 35s 166ms/step - loss: 0.7185 - accuracy: 0.7434 - val_loss: 0.7868 - val_accuracy: 0.7237\n",
      "Epoch 7/10\n",
      "211/211 [==============================] - 35s 164ms/step - loss: 0.6575 - accuracy: 0.7613 - val_loss: 0.7216 - val_accuracy: 0.7393\n",
      "Epoch 8/10\n",
      "211/211 [==============================] - 33s 155ms/step - loss: 0.6331 - accuracy: 0.7769 - val_loss: 0.6349 - val_accuracy: 0.7713\n",
      "Epoch 9/10\n",
      "211/211 [==============================] - 33s 156ms/step - loss: 0.5792 - accuracy: 0.7924 - val_loss: 0.7573 - val_accuracy: 0.7304\n",
      "Epoch 10/10\n",
      "211/211 [==============================] - 33s 157ms/step - loss: 0.5600 - accuracy: 0.8059 - val_loss: 0.7042 - val_accuracy: 0.7527\n",
      "Test loss: 0.7042356133460999\n",
      "Test accuracy: 0.7526666522026062\n"
     ]
    }
   ],
   "source": [
    "model_5.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model_5.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model_5.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S7:\n",
    "\n",
    "- Apply your best model on multispectral images. You may use whichever image channels you wish, so long as you use more than just RGB (although you are not required to use any color channels).\n",
    "\n",
    "Q7.1: Calculate classification accuracy on the test data.\n",
    "\n",
    "Q7.2: Compare against results using RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1152,  830,  668, ..., 1592,  592, 3894],\n",
       "        [1152,  830,  668, ..., 1592,  592, 3894],\n",
       "        [1152,  851,  679, ..., 1607,  593, 3951],\n",
       "        ...,\n",
       "        [1153,  813,  588, ...,  828,  335, 2060],\n",
       "        [1153,  811,  618, ...,  830,  344, 2044],\n",
       "        [1153,  820,  615, ...,  844,  360, 2055]],\n",
       "\n",
       "       [[1152,  830,  668, ..., 1592,  592, 3894],\n",
       "        [1152,  830,  668, ..., 1592,  592, 3894],\n",
       "        [1152,  851,  679, ..., 1607,  593, 3951],\n",
       "        ...,\n",
       "        [1153,  813,  588, ...,  828,  335, 2060],\n",
       "        [1153,  811,  618, ...,  830,  344, 2044],\n",
       "        [1153,  820,  615, ...,  844,  360, 2055]],\n",
       "\n",
       "       [[1153,  839,  670, ..., 1593,  592, 3874],\n",
       "        [1153,  839,  670, ..., 1593,  592, 3874],\n",
       "        [1153,  846,  672, ..., 1604,  592, 3931],\n",
       "        ...,\n",
       "        [1154,  832,  608, ...,  851,  338, 2082],\n",
       "        [1154,  832,  627, ...,  864,  352, 2088],\n",
       "        [1154,  811,  619, ...,  887,  374, 2123]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1149,  822,  623, ..., 1437,  551, 3385],\n",
       "        [1149,  822,  623, ..., 1437,  551, 3385],\n",
       "        [1149,  819,  617, ..., 1487,  558, 3534],\n",
       "        ...,\n",
       "        [1151,  827,  625, ..., 1259,  468, 3037],\n",
       "        [1151,  821,  630, ..., 1235,  454, 3029],\n",
       "        [1151,  828,  612, ..., 1231,  451, 3052]],\n",
       "\n",
       "       [[1150,  813,  604, ..., 1400,  537, 3303],\n",
       "        [1150,  813,  604, ..., 1400,  537, 3303],\n",
       "        [1149,  844,  675, ..., 1462,  549, 3504],\n",
       "        ...,\n",
       "        [1151,  824,  644, ..., 1207,  448, 2866],\n",
       "        [1151,  822,  673, ..., 1192,  436, 2873],\n",
       "        [1152,  810,  625, ..., 1194,  435, 2923]],\n",
       "\n",
       "       [[1150,  783,  591, ..., 1395,  538, 3297],\n",
       "        [1150,  783,  591, ..., 1395,  538, 3297],\n",
       "        [1150,  838,  689, ..., 1450,  550, 3503],\n",
       "        ...,\n",
       "        [1152,  809,  619, ..., 1183,  437, 2772],\n",
       "        [1152,  824,  613, ..., 1185,  430, 2804],\n",
       "        [1152,  813,  600, ..., 1197,  432, 2883]]], dtype=uint16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIRECTORY = r'EuroSATallBands\\\\ds\\\\images\\\\remote_sensing\\\\otherDatasets\\\\sentinel_2\\\\tif'\n",
    "\n",
    "CATEGORIES = ['SeaLake', 'River','Residential','PermanentCrop',\n",
    "              'Pasture','Industrial','Highway','HerbaceousVegetation',\n",
    "             'Forest','AnnualCrop']\n",
    "num_class=(len(CATEGORIES))\n",
    "image=io.imread('Forest_1.tif')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "X=[]\n",
    "Y=[]\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DIRECTORY, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        label = CATEGORIES.index(category)\n",
    "        arr = io.imread(img_path)\n",
    "#         new_arr = cv2.resize(arr, (64, 64))\n",
    "        X.append(arr)\n",
    "        Y.append(label)\n",
    "        data.append([arr, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(X).astype('float32')\n",
    "X.shape\n",
    "X_value=X[:,:,:,:4]\n",
    "X_value/=255\n",
    "Y_value=np.array(Y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_value, Y_value, stratify = Y_value, train_size = 0.5, random_state=42)\n",
    "y_train = to_categorical(y_train, num_class)\n",
    "y_test = to_categorical(y_test, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 62, 62, 32)        1184      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 31, 31, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 31, 31, 32)        0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 14, 14, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 12, 12, 64)        36928     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                92170     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 148,778\n",
      "Trainable params: 148,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6 = Sequential()\n",
    "model_6.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 4)))\n",
    "model_6.add(MaxPooling2D((2, 2)))\n",
    "model_6.add(Dropout(0.25))\n",
    "model_6.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_6.add(MaxPooling2D((2, 2)))\n",
    "model_6.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_6.add(Flatten())\n",
    "model_6.add(Dense(num_class, activation='softmax'))\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13500, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 64, 64, 4), found shape=(None, 64, 64, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model_6\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      2\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mRMSprop(),\n\u001b[0;32m      3\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_6\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m score \u001b[38;5;241m=\u001b[39m model_6\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss:\u001b[39m\u001b[38;5;124m'\u001b[39m, score[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Xiaoyi WU\\.conda\\envs\\py38\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_3\" is incompatible with the layer: expected shape=(None, 64, 64, 4), found shape=(None, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "model_6.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model_6.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=1,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "score = model_6.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
